{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJQT0wwvL2fc2QtSI4TssF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nobel712/Image-Segmentation/blob/main/Unet_%2Battention_gate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UexKatALqMYv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.layers as L\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def conv_block(x, num_filters):\n",
        "    x = L.Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = L.BatchNormalization()(x)\n",
        "    x = L.Activation(\"relu\")(x)\n",
        "\n",
        "    x = L.Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = L.BatchNormalization()(x)\n",
        "    x = L.Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder_block(x, num_filters):\n",
        "    x = conv_block(x, num_filters)\n",
        "    p = L.MaxPool2D((2, 2))(x)\n",
        "    return x, p\n",
        "\n",
        "def attention_gate(g, s, num_filters):\n",
        "    Wg = L.Conv2D(num_filters, 1, padding=\"same\")(g)\n",
        "    Wg = L.BatchNormalization()(Wg)\n",
        "\n",
        "    Ws = L.Conv2D(num_filters, 1, padding=\"same\")(s)\n",
        "    Ws = L.BatchNormalization()(Ws)\n",
        "\n",
        "    out = L.Activation(\"relu\")(Wg + Ws)\n",
        "    out = L.Conv2D(num_filters, 1, padding=\"same\")(out)\n",
        "    out = L.Activation(\"sigmoid\")(out)\n",
        "\n",
        "    return out * s\n",
        "\n",
        "def decoder_block(x, s, num_filters):\n",
        "    x = L.UpSampling2D(interpolation=\"bilinear\")(x)\n",
        "    s = attention_gate(x, s, num_filters)\n",
        "    x = L.Concatenate()([x, s])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x\n",
        "\n",
        "def attention_unet(input_shape):\n",
        "    \"\"\" Inputs \"\"\"\n",
        "    inputs = L.Input(input_shape)\n",
        "\n",
        "    \"\"\" Encoder \"\"\"\n",
        "    s1, p1 = encoder_block(inputs, 32)\n",
        "    s2, p2 = encoder_block(p1, 64)\n",
        "    s3, p3 = encoder_block(p2, 128)\n",
        "\n",
        "    b1 = conv_block(p3, 256)\n",
        "\n",
        "    \"\"\" Decoder \"\"\"\n",
        "    d1 = decoder_block(b1, s3, 128)\n",
        "    d2 = decoder_block(d1, s2, 64)\n",
        "    d3 = decoder_block(d2, s1, 32)\n",
        "\n",
        "    \"\"\" Outputs \"\"\"\n",
        "    outputs = L.Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d3)\n",
        "\n",
        "    \"\"\" Model \"\"\"\n",
        "    model = Model(inputs, outputs, name=\"Attention-UNET\")\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_shape = (256, 256, 3)\n",
        "    model = attention_unet(input_shape)\n",
        "    model.summary()"
      ],
      "metadata": {
        "id": "PxBm5-2tkep-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}